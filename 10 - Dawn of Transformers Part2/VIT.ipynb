{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2IbbxpMqvuI"
      },
      "outputs": [],
      "source": [
        "# Importing Necessary libraris\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim \n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing data, setting configuration for the device.\n",
        "DATA_DIR = './data'\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
        "print(device,\" is  available!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHWunZyrsXf7",
        "outputId": "a8052d5f-ee18-41ac-f376-d6ce215f2537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda  is  available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Residual network to add at the end of output.\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self,*layers):\n",
        "    super().__init__()\n",
        "    self.residual = nn.Sequential(*layers)\n",
        "    self.gamma = nn.Parameter(torch.zeros(1))\n",
        "    # nn.Parameter means, the parameter in the paranthesis will be considered as trainable parameter which isn't the one before\n",
        "    # nn.Parameter is a class in the PyTorch neural network module (torch.nn) that is used to create trainable parameters within a neural network model.\n",
        "    # This means that it will be included in the computation of gradients during backpropagation and can be optimized by an optimizer.\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x + self.gamma*self.residual(x) \n",
        "    # Adding residue at the end of output layer.\n"
      ],
      "metadata": {
        "id": "pC5usGrnsqy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Layernormalization to the channels.\n",
        "class LayerNormChannels(nn.Module):\n",
        "  def __init__(self,channels):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(channels) \n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x.transpose(1,-1) \n",
        "    x = self.norm(x) \n",
        "    x = x.transpose(-1,1) \n",
        "    return x "
      ],
      "metadata": {
        "id": "B-0m0fUYuIgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describing the Self-Attention Mechanism Process \n",
        "# STEP1: splitting the fully connected layers obtained after making them linear from patches in to heads by dividing with number of head channels\n",
        "# STEP2: and specifying the key, Query, Value parameters.\n",
        "# STEP3: Inventing the position encoding(injecting number for patch) so that we can add it position embedddings.\n",
        "# Forward:\n",
        "# STEP1: Defining Key, Query, Value by viewing them as in shape of (batch_size,(width,1),number_of_head_channels,-1)\n",
        "# STEP2: Calculating attention score and applying softmax, and calc. attention matrix and returning output \n",
        "# StaticMethod: Getting indices.\n",
        "\n",
        "\n",
        "class SelfAttention2d(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,head_channels,shape):\n",
        "    super().__init__()\n",
        "    # STEP1:\n",
        "    # 512/8\n",
        "    self.heads = out_channels //head_channels\n",
        "    self.head_channels = head_channels \n",
        "    self.scale = head_channels**-0.5 \n",
        "    # STEP2:\n",
        "    # Calc. hnumber of heads, defining keys,queries,values, unifyheads\n",
        "    self.to_keys = nn.Conv2d(in_channels,out_channels,1) \n",
        "    self.to_queries = nn.Conv2d(in_channels,out_channels,1) \n",
        "    self.to_values = nn.Conv2d(in_channels,out_channels,1) \n",
        "    self.unifyheads = nn.Conv2d(in_channels,out_channels,1) \n",
        "    # STEP3: \n",
        "    height,width = shape \n",
        "    self.pos_enc = nn.Parameter(torch.Tensor(self.heads,(2*height-1)*(2*width-1)))\n",
        "    # (8,16*16)\n",
        "    # , register_buffer() is a method that allows you to register a tensor as a buffer parameter of a PyTorch module.\n",
        "    # Buffers are tensor-like objects that are not considered model parameters and therefore are not updated during backpropagation.\n",
        "    #  They are often used to store non-learnable parameters that are associated with a model, such as running statistics used for normalization layers.\n",
        "    self.register_buffer(\"relative_indices\",self.get_indices(height,width)) \n",
        "\n",
        "  def forward(self,x):\n",
        "    b,_,h,w = x.shape \n",
        "    # STEP1:\n",
        "    keys = self.to_keys(x).view(b,self.heads,self.head_channels,-1)\n",
        "    values = self.to_values(x).view(b,self.heads,self.head_channels,-1)\n",
        "    queries = self.to_queries(x).view(b,self.heads,self.head_channels,-1)\n",
        "    # STEP2:\n",
        "    attention_score = keys.transpose(-2,-1)@queries \n",
        "    indices = self.relative_indices.expand(self.heads,-1) \n",
        "    rel_pos_enc = self.pos_enc.gather(-1,indices) \n",
        "    print('re',rel_pos_enc.shape)\n",
        "    rel_pos_enc = rel_pos_enc.unflatten(-1,(h*w,h*w)) \n",
        "    print('re flat',rel_pos_enc.shape)\n",
        "    attention_score  = attention_score* self.scale + rel_pos_enc \n",
        "    attention_score = F.softmax(attention_score,dim=-2) \n",
        "\n",
        "    out = values @ attention_score \n",
        "    out = out.view(b,-1,h,w) \n",
        "    out = self.unifyheads(out)  \n",
        "    return out \n",
        "  \n",
        "  @staticmethod \n",
        "  def get_indices(h,w):\n",
        "    y = torch.arange(h,dtype=torch.long) \n",
        "    x = torch.arange(w,dtype=torch.long) \n",
        "    y1,x1,y2,x2 = torch.meshgrid(y,x,y,x,indexing='ij')\n",
        "    indices = (y1-y2+h-1)*(2*w-1) + x1-x2 + w-1 \n",
        "    indices = indices.flatten() \n",
        "    return indices    \n",
        "\n"
      ],
      "metadata": {
        "id": "XvL2qynHwZZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed Forward N/w\n",
        "# numberofhiddenlayers = in_channels * mult \n",
        "# A Convolution layer is built on top resulting in in_channels & producing hidden_channels outputs\n",
        "# And again in-taking hidden_channels as number of input_channels to consider and result in out_channels number\n",
        "class FeedForward(nn.Sequential):\n",
        "  def __init__(self,in_channels,out_channels,mult=4):\n",
        "    hidden_channels = in_channels*mult \n",
        "    super().__init__(\n",
        "        nn.Conv2d(in_channels,hidden_channels,1),\n",
        "        nn.GELU(),\n",
        "        nn.Conv2d(hidden_channels,out_channels,1) \n",
        "    )"
      ],
      "metadata": {
        "id": "NdntCqC11y_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A Transformer Block Where we group:\n",
        "## A Residual Block: Attention2d + residue \n",
        "## A Residual Block: LayerNormalization o/p + FeedForwarded O/p (Conv + Conv) (i,e,,class FeedForward)\n",
        "class TransformerBlock(nn.Sequential): \n",
        "  def __init__(self,channels,head_channels,shape,p_drop=0.):\n",
        "    super().__init__(\n",
        "        Residual(\n",
        "            LayerNormChannels(channels),\n",
        "            SelfAttention2d(channels,channels,head_channels,shape),\n",
        "            nn.Dropout(p_drop)\n",
        "        ),\n",
        "        Residual(\n",
        "            LayerNormChannels(channels),\n",
        "            FeedForward(channels,channels), \n",
        "            nn.Dropout(p_drop)\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "YuTS0jrr2JqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Stacking the transformer block num_block times as a list \n",
        "class TransformerStack(nn.Sequential):\n",
        "  def __init__(self,num_blocks,channels,head_channels,shape,p_drop=0.):\n",
        "    layers = [TransformerBlock(channels,head_channels,shape,p_drop) for _ in range(num_blocks)]\n",
        "    super().__init__(*layers)"
      ],
      "metadata": {
        "id": "owWLoN-f2kqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## To split the image in patches by convolving with patch_size and a stride of patch_size \n",
        "class ToPatches(nn.Sequential):\n",
        "  def __init__(self,in_channels,channels,patch_size,hidden_channels=32):\n",
        "    super().__init__(\n",
        "        nn.Conv2d(in_channels,hidden_channels,3,padding = 1), \n",
        "        nn.GELU(), \n",
        "        nn.Conv2d(hidden_channels,channels,patch_size,stride=patch_size)\n",
        "    )"
      ],
      "metadata": {
        "id": "PbaJsjv825W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Adding Position Embedding to existing input of forward function \n",
        "class AddPositionEmbedding(nn.Module):\n",
        "  def __init__(self,channels,shape):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = nn.Parameter(torch.Tensor(channels,*shape))\n",
        "  \n",
        "  def forward(self,x):\n",
        "    print(x.shape)\n",
        "    # 128,32,16,16\n",
        "    print('pos',self.pos_embedding.shape) # 32,16,16\n",
        "    y = x + self.pos_embedding\n",
        "    # 128,32,16\n",
        "    print('after_pos',y.shape)\n",
        "    return x+self.pos_embedding "
      ],
      "metadata": {
        "id": "DZeuk2hI3TlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a one of main block which involves the processes:\n",
        "## A. Splitting the image into patches\n",
        "## B. Adding Position Embedding\n",
        "## This involves both codes and do operations\n",
        "class ToEmbedding(nn.Sequential): \n",
        "  def __init__(self,in_channels,channels,patch_size,shape,p_drop=0.):\n",
        "    super().__init__(\n",
        "        # in_channels = 3, channels = 32, shape = (16,16)\n",
        "        ToPatches(in_channels,channels,patch_size), \n",
        "        AddPositionEmbedding(channels,shape),\n",
        "        nn.Dropout(p_drop)\n",
        "    )\n",
        "    "
      ],
      "metadata": {
        "id": "C45scrZf3in5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Head, Where it does layer normalizations and use a gap layer to flatten it out and again convert to FC layer with  flattened linear layer as a num of classes layer.\n",
        "class Head(nn.Sequential):\n",
        "  def __init__(self,in_channels,classes,p_drop=0.):\n",
        "    super().__init__(\n",
        "        LayerNormChannels(in_channels), \n",
        "        nn.GELU(), \n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        nn.Flatten(), \n",
        "        nn.Dropout(p_drop),\n",
        "        nn.Linear(in_channels,classes)\n",
        "    )"
      ],
      "metadata": {
        "id": "wFrrx63J3zn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Main Class where it takes all of inputs and involves all the codes above one way or other \n",
        "class ViT(nn.Sequential):\n",
        "  def __init__(self,classes,image_size,channels,head_channels,num_blocks,patch_size,in_channels=3,emb_p_drop=0.,trans_p_drop=0.,head_p_drop=0.):\n",
        "    reduced_size = image_size//patch_size \n",
        "    shape = (reduced_size,reduced_size) \n",
        "    \n",
        "    # Involving process of embedding and stacking transformer layers and takes the output and result in num_of_classes as final output.\n",
        "    # Process From Input to Output # Making to patches --> Stacking to transformers --> Concatentating all layers and resulting in num_classes as o/p\n",
        "    super().__init__(\n",
        "        ToEmbedding(in_channels,channels,patch_size,shape,emb_p_drop),\n",
        "        TransformerStack(num_blocks,channels,head_channels,shape,trans_p_drop),\n",
        "        Head(channels,classes,head_p_drop))\n",
        "    self.reset_parameters() \n",
        "\n",
        "  def reset_parameters(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m,(nn.Conv2d,nn.Linear)):\n",
        "        nn.init.kaiming_normal_(m.weight) \n",
        "        if m.bias is not None:\n",
        "          nn.init.zeros_(m.bias) \n",
        "      elif isinstance(m,nn.LayerNorm):\n",
        "        nn.init.constant_(m.weight,1.) \n",
        "        nn.init.zeros_(m.bias) \n",
        "      elif isinstance(m,AddPositionEmbedding): \n",
        "        nn.init.normal_(m.pos_embedding,mean=0.0,std=0.02)\n",
        "      elif isinstance(m,SelfAttention2d):\n",
        "        nn.init.normal_(m.pos_enc,mean=0.0,std=0.02)\n",
        "      elif isinstance(m,Residual):\n",
        "        nn.init.zeros_(m.gamma) \n",
        "\n",
        "  def separate_parameters(self):\n",
        "    parameters_decay = set() \n",
        "    parameters_no_decay = set() \n",
        "    modules_weight_decay = (nn.Linear,nn.Conv2d)\n",
        "    modules_no_weight_decay = (nn.LayerNorm,)\n",
        "\n",
        "    for m_name,m in self.named_modules():\n",
        "      for param_name,param in m.named_parameters():\n",
        "        full_param_name = f\"{m_name}.{param_name}\" if m_name else param_name \n",
        "\n",
        "        if isinstance(m,modules_no_weight_decay):\n",
        "          parameters_no_decay.add(full_param_name) \n",
        "        elif param_name.endswith(\"bias\"):\n",
        "          parameters_no_decay.add(full_param_name) \n",
        "        elif isinstance(m,Residual) and param_name.endswith(\"gamma\"):\n",
        "          parameters_no_decay.add(full_param_name) \n",
        "        elif isinstance(m,AddPositionEmbedding) and param_name.endswith(\"pos_embedding\"): \n",
        "          parameters_no_decay.add(full_param_name)\n",
        "        elif isinstance(m,selfAttention2d) and param_name.endswith(\"pos_enc\"):\n",
        "          parameters_no_decay.add(full_param_name) \n",
        "        elif isinstance(m,modules_weight_decay):\n",
        "          parameters_decay.add(full_param_name) \n",
        "    # Sanity Check \n",
        "    # assert len(parameters_decay & parameters_no_decay) == 0 \n",
        "    # asser len(parameters_decay) + len(parameters_no_decay) == len(list(model.parameters()))\n",
        "\n",
        "    return parameters_decay,parameters_no_decay"
      ],
      "metadata": {
        "id": "4ydVDwel4EtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES,IMAGE_SIZE = 10,32\n",
        "model = ViT(NUM_CLASSES, IMAGE_SIZE, channels=32, head_channels=8, num_blocks=4, patch_size=2,\n",
        "               emb_p_drop=0., trans_p_drop=0., head_p_drop=0.1)"
      ],
      "metadata": {
        "id": "bjXGMqxs6L23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_EWEaUe8gsP",
        "outputId": "b18fe09e-4139-4eeb-9d38-cc467e1d10a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViT(\n",
              "  (0): ToEmbedding(\n",
              "    (0): ToPatches(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (1): AddPositionEmbedding()\n",
              "    (2): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (1): TransformerStack(\n",
              "    (0): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (0): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): SelfAttention2d(\n",
              "            (to_keys): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_queries): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (to_values): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (unifyheads): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): Residual(\n",
              "        (residual): Sequential(\n",
              "          (0): LayerNormChannels(\n",
              "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (1): FeedForward(\n",
              "            (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (1): GELU(approximate='none')\n",
              "            (2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (2): Head(\n",
              "    (0): LayerNormChannels(\n",
              "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): GELU(approximate='none')\n",
              "    (2): AdaptiveAvgPool2d(output_size=1)\n",
              "    (3): Flatten(start_dim=1, end_dim=-1)\n",
              "    (4): Dropout(p=0.1, inplace=False)\n",
              "    (5): Linear(in_features=32, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isUk96nT8k2g",
        "outputId": "c742109a-da25-48cb-f1c9-c17eb657be45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 79,810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 32\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 25\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1\n",
        "\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32, scale=(0.75, 1.0), ratio=(1.0, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandAugment(num_ops=1, magnitude=8),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std),\n",
        "    transforms.RandomErasing(p=0.25)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eerqlO88qY7",
        "outputId": "ee09aacb-8e6d-4faf-c4fd-86d39545fbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# GradScaler is used for computing gradients and updating carefully such that no overflow and underflow happen.\n",
        "# autocast, which converts high valued floating point numbers in to lower data type floating points such that to efficiently use memory and improve performance>\n",
        "\n",
        "clip_norm = True\n",
        "lr_schedule = lambda t: np.interp([t], [0, EPOCHS*2//5, EPOCHS*4//5, EPOCHS], \n",
        "                                  [0, 0.01, 0.01/20.0, 0])[0]\n",
        "\n",
        "model = nn.DataParallel(model, device_ids=[0]).cuda()\n",
        "opt = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    train_loss, train_acc, n = 0, 0, 0\n",
        "    for i, (X, y) in enumerate(trainloader):\n",
        "        model.train()\n",
        "        X, y = X.cuda(), y.cuda() \n",
        "        lr = lr_schedule(epoch + (i + 1)/len(trainloader))\n",
        "        opt.param_groups[0].update(lr=lr)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        if clip_norm:\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item() * y.size(0)\n",
        "        train_acc += (output.max(1)[1] == y).sum().item()\n",
        "        n += y.size(0)\n",
        "        \n",
        "    model.eval()\n",
        "    test_acc, m = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(testloader):\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                output = model(X)\n",
        "            test_acc += (output.max(1)[1] == y).sum().item()\n",
        "            m += y.size(0)\n",
        "\n",
        "    print(f'ConvMixer: Epoch: {epoch} | Train Acc: {train_acc/n:.4f}, Test Acc: {test_acc/m:.4f}, Time: {time.time() - start:.1f}, lr: {lr:.6f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XRuY9ut38r3_",
        "outputId": "564326ba-c8e0-419b-a700-e3c65b059bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "torch.Size([128, 32, 16, 16])\n",
            "pos torch.Size([32, 16, 16])\n",
            "after_pos torch.Size([128, 32, 16, 16])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n",
            "re torch.Size([4, 65536])\n",
            "re flat torch.Size([4, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-6d1bba8e0742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BCbDdm8N8tbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}